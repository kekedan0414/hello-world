**【什么是机器学习】**
机器学习最通俗的解释就是让机器学会决策。再稍微具体点，就是通过喂给机器一定量的历史数据，得到一个可以决策的模型，当
有新数据过来的时候，通过模型可以给出预测。
从预测值的类型上看，连续变量预测的定量输出称为回归；离散变量预测的定性输出称为分类。例如：预测明天多少度，是一个回归任务；预测明天阴、晴、雨，就是一个分类任务。

**【有哪些应用】**
1、防垃圾邮件系统
2、推荐系统
3、车牌号识别
4、人脸识别
5、语音识别
6、图像识别

**【智能投顾】**

**【防欺诈模型】**
构建知识图谱（存放图数据库Neo4j），挖掘关系网络中隐藏的欺诈风险。
从算法的角度来讲，有两种不同的场景：一种是基于规则的；另一种是基于概率的。

基于规则：1、不一致性验证，通过规则找到矛盾点，比如张三李四在不同公司上班，但是拥有相同的电话号码。
          2、基于规则提取特征，申请人二度人脉关系中有多少进了黑名单。
数据量多的时候：
基于概率：比如社区挖掘（），标签传播（优秀的人有的特质，朋友也可能会有），

**【人工智能应用】**

**【监督式学习】**
 在监督式学习下，输入数据被称为“训练数据”，每组训练数据有一个明确的标识或结果，例如，防垃圾邮件系统，识别手写数字识等。

**【线性回归】**
解决回归问题，例如通过目前已有的数据来画一条线，然后希望新的数据也能尽可能落在这条线上。
通过损失函数计算误差，并且利用梯度下降法，逐步最小化损失函数。

**【最小二乘】**
线性回归的一种计算方法，每个点到直线上的垂直距离，加起来值最小，那么这条线就是拟合最好的线。

**【逻辑回归】**
虽然带有回归字样，但是属于分类算法，多用于二分类，结果取值[0-1],以0.5为分界，大于0.5的为一类，小于0.5为一类，
逻辑回归公式就是sigmoid。

**【Knn近邻算法】**
给一个新的数据时，离它最近的 k 个点中，哪个类别多，这个新数据就属于哪一类。
选k个邻居，然后统计哪个类别的邻居最多，那么我就属于哪一类。关键点：选择合适的K值。
例子：要区分两个类别A和B，设置k=3，那么取三个最近的点，A类有2个，B类有1个，那么这个点就属于A类。

**【K-Means均值算法】**聚类
先将一组数据分为3（K类）类。
第一步在这组数据中随机选3个点作为3个原点，然后计算每个点到这3个原点的距离。离哪个原点最近，就属于哪个类别。
第二步再计算着三个类的平均值作为新的原点。然后再计算所有点分别离这3个原点的距离，如果一个点距离某个原点最近，那么属于那一类。
然后反复迭代，不停的计算新原点，不停的重新聚类，直到结果相对收敛。

**【决策树】**
决策树通常有三个步骤：特征选择、决策树的生成、决策树的修剪。
一些概念：信息增益  = 经验熵 - 条件熵。
根据信息增益来可以确定谁是最优特征，最优特征为根。
由于决策树生成后，对于训练样本来说是过拟合的，因此通过损失函数来进行修剪，提高决策树的泛化能力。

**【随机森林】**
集成学习（ensemble）思想是为了解决单个模局限性缺陷，从而整合起更多的模型，取长补短，避免局限性。
森林是指有多颗决策树，而随机是指每颗决策树的特征条件是随机选择的，是不一样的。
假设样本总数为M，特征个数为F，
选择样本个数N<M,
选择特征a<F，
选择决策树颗数为K。
创建时，每个决策树随机选定数目为a的条件，随即选定数目为N的样本。
创建完成后，将测试样本带入森林，每颗决策树会给出决策，然后汇总并做最终决策。并与实际结果做比较。
调参：理论上来说，随机森林中树的数目越多，模型的效果就越好。但这种效果是递减的，而且消耗计算资源，
因此需要取一个合适的值。

**【朴素贝叶斯】**
有一个贝叶斯公式。主要描述的是后验概率，即在已知某种条件发生后，事件的发生概率是多少？
P(A|B) = P(A)P(B|A)/P(B)
通常用于NLP自然语言处理中。比如垃圾邮件分类等。
假设样本x（特征f1，f2，f3）需要分类。
类别y1，y2.
求x属于y1还是y2。
P（y1|x） = P（y1）P（x|y1）/P（x）
P（y2|x） = P（y2）P（x|y2）/P（x）
分母一样可以不计算。
并且朴素的意思就是假设没有个特征都是相互独立的。那么：
P（y1|x） = P（y1）P（f1|y1）P（f2|y1）P（f3|y1）
P（y2|x） = P（y2）P（f1|y2）P（f2|y2）P（f3|y2）
而P（y1），P（y2），P（f1|y1），P（f1|y2）等这些都可以在统计资料里得到统计概率。计算谁大，就属于那一类了。

**【SVM支持向量机】**
支持向量机（support vector machines）是一种二分类模型，它的目的是寻找一个超平面来对样本进行分割，而对于分割来说，有
无数种分法，。那么支持向量机的分割原则是间隔最大化，就是选择两个样本边界的正中间作为划分界限。
这样对于样本的扰动容忍性最好。如果划分的不好，在边界附近的新样本很容易错误的归类。

**【协同过滤collaborative filtering CF】**
推荐系统常用，
基于用户的协同过滤。用户A和B很像，将A喜欢的东西推荐给B。
基于物品的协同过滤。物品A和物品B很像。那么我就将B推荐给喜欢A的人。

如何衡量这个相似呢，主要有几种距离评价算法。欧氏距离，曼哈顿距离。